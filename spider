#! /bin/bash
# Crawl a site to 'prime' the frontend cache. Outputs log and total time taken to stderr.
# It will not spawn multiple times (locking mechanism). Thus it is save calling it in an often-running cronjob. 
#
# @param URL $1
# @param Depth $2
# @param User $3 (HTTP-Auth)
# @param Password $4 (HTTP-Auth)
# @param Cookiefile (see wget --load-cookies)

set -o errexit -o pipefail -o noclobber -o nounset

url="${1-}"
if [ -z "$url" ]; then
  echo 'ERROR: no URL given'
  exit 1
fi

depth="${2-3}"
if [ "$depth" == "0" ]; then
  recursive=""
else
  recursive="--recursive"
fi

user="${3-}"
password="${4-}"

cookieFile="${5-}"
if [ -z "$cookieFile" ]; then
  loadCookies=""
else
  loadCookies="--load-cookies=\"$cookieFile\""
fi

function shutdown() {
  [ -d "$tmpDir" ] && rm -rf "$tmpDir"
  [ -f "$lockFile" ] && rm "$lockFile"
}
trap shutdown EXIT INT TERM
tmpDir="$(mktemp -d)"
cd "$tmpDir"


urlSanitized=$(echo "$url$depth" | sed -r s/[:/]/_/g)

lockFile="/var/lock/$(basename "$0")_${urlSanitized}"
{
  if ! flock -n 9; then
    echo "Already running (found lock $lockFile), exiting" 2>&1
    exit 1
  fi

  echo STARTED "$(date -Is -u)"
  cmd="wget \
    --header 'User-Agent: Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:51.0) Gecko/20100101 Firefox/51.0' \
    --no-check-certificate --no-verbose --execute robots=off \
    --reject '*.*' \
    --no-directories $recursive $loadCookies \
    --delete-after --level=\"$depth\" \
    --user=\"$user\" --password=\"$password\" \"$url\""
  time eval "$cmd"

} 9>"$lockFile" 

# vim: syntax=bash ts=2 sw=2 et:
