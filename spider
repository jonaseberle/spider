#! /bin/bash
# Crawl a site to 'prime' the frontend cache. Outputs log and total time taken to stderr.
# It will not spawn multiple times (locking mechanism). Thus it is save calling it in an often-running cronjob. 
#
# @param URL $1
# @param Depth $2
# @param User $3 (HTTP-Auth)
# @param Password $4 (HTTP-Auth)
# @param Cookiefile (see wget --load-cookies)

set -o errexit -o pipefail -o noclobber -o nounset

url="${1-}"
if [ -z "$url" ]; then
        echo 'ERROR: no URL given'
        exit 1
fi

depth="${2-3}"
if [ "$depth" == "0" ]; then
        recursive=""
else
        recursive="--recursive"
fi

user="${3-}"
password="${3-}"

cookieFile="${5-}"
if [ -z "$cookieFile" ]; then
        loadCookies=""
else
        loadCookies="--load-cookies=\"$cookieFile\""
fi

function shutdown() {
  [ -d "$tmpDir" ] &&  rm -rf "$tmpDir"
}
trap shutdown INT
tmpDir="$(mktemp -d)"
cd "$tmpDir"


this=$(readlink -f $0)
thisDir=$(dirname $this)
thisDirBase=$thisDir/$(basename $0)
urlSanitized=$(echo $url$depth | sed -r s/[:/]/_/g)

lockFile=/var/lock/$(basename $0)_${urlSanitized}
{
        if ! flock -n 9; then
                #echo "Unable to lock $lockFile, exiting" 2>&1
                exit 1
        fi

        # remove lockfile
        trap "rm $lockFile 2> /dev/null" EXIT INT KILL TERM

        echo STARTED $(date -Is -u)
        time \
		wget $loadCookies \
			--header 'User-Agent: Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:51.0) Gecko/20100101 Firefox/51.0' \
			--no-check-certificate --no-verbose --execute robots=off \
			--reject '*.*' \
			--no-directories $recursive \
			--delete-after --level=$depth \
			--user="$user" --password="$password" "$url"
		
} 9>"$lockFile" 

